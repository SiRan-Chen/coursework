labs(title = "MSARIMA 模型预测 vs 实际值",
x = "时间（小时）", y = "车速（km/h）") +
theme_minimal()
# 加载必要包
library(ggplot2)
library(forecast)
# 创建预测图，加入图例
p <- autoplot(fc, series = "预测值") +
autolayer(test_ts, series = "实际值", color = "red") +
labs(title = "MSARIMA 模型预测 vs 实际值",
x = "时间（小时）", y = "车速（km/h）") +
scale_color_manual(values = c("预测值" = "blue", "实际值" = "red")) +
guides(color = guide_legend(title = "图例")) +
theme_minimal()
# 保存为高质量 PNG
ggsave(filename = "./data/processed/prediction_vs_actual_with_legend.png",
plot = p,
width = 10, height = 5, dpi = 300)
# 保存图像为 PNG 文件
ggsave(filename = "./data/processed/prediction_vs_actual.png",
plot = p,
width = 10, height = 5, dpi = 300)
# 加载所需包
library(forecast)
# 创建训练和测试数据（假设 ts_data 已是 frequency=168）
train_ts <- window(ts_data, start = c(1, 1), end = c(6, 0))   # 前6周（1008小时）
test_ts <- window(ts_data, start = c(6, 1))                   # 后2周（336小时）
# 拟合双季节 Holt-Winters 模型（multiplicative 推荐）
fit_dshw <- dshw(train_ts, period1 = 24, period2 = 168, seasonal = "multiplicative")
# 加载所需包
library(forecast)
# 创建训练和测试数据（假设 ts_data 已是 frequency=168）
train_ts <- window(ts_data, start = c(1, 1), end = c(6, 0))   # 前6周（1008小时）
test_ts <- window(ts_data, start = c(6, 1))                   # 后2周（336小时）
# 拟合双季节 Holt-Winters 模型（默认使用乘法形式）
fit_dshw <- dshw(train_ts, period1 = 24, period2 = 168)
# 预测未来 336 小时（2周）
fc_dshw <- forecast(fit_dshw, h = length(test_ts))
# 加载所需包
library(forecast)
library(Metrics)
library(ggplot2)
# 构建训练集（前6周 = 1008小时）
train_ts <- window(ts_data, start = c(1, 1), end = c(6, 0))
# 真实测试集为下一周（168小时）
test_ts <- window(ts_data, start = c(6, 1), end = c(7, 0))
# 拟合 DSHW 模型并设置预测长度为 168 小时（1周）
fit_dshw <- dshw(train_ts, period1 = 24, period2 = 168, h = 168)
# 直接使用模型结果作为预测结果
fc_dshw <- fit_dshw  # dshw 返回的对象已包含预测
# 可视化
autoplot(fc_dshw) +
autolayer(test_ts, series = "实际值", color = "red") +
labs(title = "DSHW 模型预测 vs 实际值（未来一周）",
x = "时间（小时）", y = "车速（km/h）") +
guides(color = guide_legend(title = "图例")) +
theme_minimal()
# 预测值 vs 实际值
pred_dshw <- fc_dshw$mean
actual <- as.numeric(test_ts)
# 计算指标
rmse_dshw <- rmse(actual, pred_dshw)
mape_dshw <- mape(actual, pred_dshw) * 100
ei_dshw <- 1 - (sqrt(sum((actual - pred_dshw)^2)) /
(sqrt(sum(actual^2)) + sqrt(sum(pred_dshw^2))))
# 输出
cat(sprintf("DSHW 预测 RMSE: %.4f\n", rmse_dshw))
cat(sprintf("DSHW 预测 MAPE: %.2f%%\n", mape_dshw))
cat(sprintf("DSHW 预测 EI: %.4f\n", ei_dshw))
# 保存为 PNG 文件（300 dpi 高分辨率）
ggsave(filename = "./data/processed/dshw_prediction_week.png",
plot = p_dshw,
width = 10, height = 5, dpi = 300)
# 直接使用模型结果作为预测结果
fc_dshw <- fit_dshw  # dshw 返回的对象已包含预测
# 可视化
autoplot(fc_dshw) +
autolayer(test_ts, series = "实际值", color = "red") +
labs(title = "DSHW 模型预测 vs 实际值（未来一周）",
x = "时间（小时）", y = "车速（km/h）") +
guides(color = guide_legend(title = "图例")) +
theme_minimal()
# 保存为 PNG 文件（300 dpi 高分辨率）
ggsave(filename = "./data/processed/dshw_prediction_week.png",
plot = p_dshw,
width = 10, height = 5, dpi = 300)
# 构建训练集（前6周 = 1008小时）
train_ts <- window(ts_data, start = c(1, 1), end = c(6, 0))
# 真实测试集为下一周（168小时）
test_ts <- window(ts_data, start = c(6, 1), end = c(7, 0))
# 拟合 DSHW 模型并设置预测长度为 168 小时（1周）
fit_dshw <- dshw(train_ts, period1 = 24, period2 = 168, h = 168)
# 直接使用模型结果作为预测结果
fc_dshw <- fit_dshw  # dshw 返回的对象已包含预测
# 可视化
autoplot(fc_dshw) +
autolayer(test_ts, series = "实际值", color = "red") +
labs(title = "DSHW 模型预测 vs 实际值（未来一周）",
x = "时间（小时）", y = "车速（km/h）") +
guides(color = guide_legend(title = "图例")) +
theme_minimal()
# 保存为 PNG 文件（300 dpi 高分辨率）
ggsave(filename = "./data/processed/dshw_prediction_week.png",
plot = p_dshw,
width = 10, height = 5, dpi = 300)
# 预测值 vs 实际值
pred_dshw <- fc_dshw$mean
actual <- as.numeric(test_ts)
# 保存为 PNG 文件（300 dpi 高分辨率）
ggsave(filename = "./data/processed/dshw_prediction_week.png",
plot = p_dshw,
width = 10, height = 5, dpi = 300)
# 可视化
autoplot(fc_dshw) +
autolayer(test_ts, series = "实际值", color = "red") +
labs(title = "DSHW 模型预测 vs 实际值（未来一周）",
x = "时间（小时）", y = "车速（km/h）") +
guides(color = guide_legend(title = "图例")) +
theme_minimal()
# 预测值 vs 实际值
pred_dshw <- fc_dshw$mean
actual <- as.numeric(test_ts)
# 保存为 PNG 文件（300 dpi 高分辨率）
ggsave(filename = "./data/processed/dshw_prediction_week.png",
plot = p_dshw,
width = 10, height = 5, dpi = 300)
# 第一步：创建图对象并赋值给 p_dshw
p_dshw <- autoplot(fc_dshw) +
autolayer(test_ts, series = "实际值", color = "red") +
labs(title = "DSHW 模型预测 vs 实际值（未来一周）",
x = "时间（小时）", y = "车速（km/h）") +
guides(color = guide_legend(title = "图例")) +
theme_minimal()
# 预测值 vs 实际值
pred_dshw <- fc_dshw$mean
actual <- as.numeric(test_ts)
# 保存为 PNG 文件（300 dpi 高分辨率）
ggsave(filename = "./data/processed/dshw_prediction_week.png",
plot = p_dshw,
width = 10, height = 5, dpi = 300)
install.packages("keras")
library(keras)
# 如果第一次使用 keras，需安装 TensorFlow 后端：
install_keras()  # 会自动下载 Python + TensorFlow
library(maptools)
install.packages("maptools")
library(maptools)
library(maptools)
install.packages("maptools")
library(sf)
# 指定路径读取 mif/mid 空间数据
spatial_data <- st_read(dsn = "path/to/your/data", layer = "R", driver = "MapInfo File")
library(sf)
spatial_data <- st_read(
dsn = "F:/ARIMA自回归滑动平均模型时间序列预测-预测未来数据/project10/data/raw/Q-Traffic Dataset/panwang/",
layer = "R",
driver = "MapInfo File"
)
library(sf)
spatial_data <- st_read(
dsn = "data/raw/Q-Traffic Dataset/panwang",
layer = "R",
driver = "MapInfo File"
)
library(sf)
library(spdep)
# spatial_data 是你用 st_read() 读取的路段数据
# poly2nb 要求是面状对象，如果是线段建议用 knearneigh 或 dnearneigh
# 使用 Queen 邻接法（边或角接触都算邻接）
nb <- poly2nb(spatial_data)
library(sf)
spatial_data <- st_read(
dsn = "data/raw/Q-Traffic Dataset/panwang",
layer = "R",
driver = "MapInfo File"
)
library(readxl)
library(dplyr)
library(purrr)
library(tidyr)
library(lubridate)
# 设置文件夹路径
data_dir <- "./data/raw/Q-Traffic Dataset/traffic_speed_sub-dataset.v2/split_files/"
# 明确列出所有xlsx文件
xlsx_files <- list.files(data_dir, pattern = "\\.xlsx$", full.names = TRUE)
# 排除以 "~" 开头的临时文件
xlsx_files <- xlsx_files[!grepl("/~\\$", xlsx_files)]
# 查看确认一下文件列表
print(xlsx_files)
read_and_process_file <- function(file_path) {
data <- read_excel(file_path, col_names = FALSE)
# 分割并规范化数据
data_clean <- data %>%
separate(col = 1, into = c("link_id", "time_index", "speed"), sep = ",", convert = TRUE) %>%
mutate(speed = as.numeric(speed),
time_index = as.integer(time_index),
link_id = as.character(link_id))
# 明确起始时间（根据数据说明文档，应自行调整确认）
start_time <- ymd_hms("2017-04-01 00:00:00")
# 转换为实际时间戳
data_clean <- data_clean %>%
mutate(time = start_time + minutes((time_index - min(time_index)) * 15))
return(data_clean)
}
library(purrr)
library(dplyr)
# 设置批次大小（建议每次处理5个文件）
batch_size <- 5
# 将所有文件路径分成多个批次
xlsx_files_batches <- split(xlsx_files, ceiling(seq_along(xlsx_files)/batch_size))
# 初始化空的数据框，存储每条道路批次的汇总结果
road_speed_summary <- data.frame()
# 开始逐批处理
for (i in seq_along(xlsx_files_batches)) {
cat("正在处理第", i, "批文件，共", length(xlsx_files_batches[[i]]), "个文件\n")
# 读取当前批次的数据
batch_data <- map_dfr(xlsx_files_batches[[i]], read_and_process_file)
# 直接计算当前批次每条道路的整体平均速度
batch_avg_speed <- batch_data %>%
group_by(link_id) %>%
summarise(
avg_speed_batch = mean(speed, na.rm = TRUE),
n_records_batch = n(),
.groups = "drop"
)
# 将批次结果合并到总汇总表中
road_speed_summary <- road_speed_summary %>%
full_join(batch_avg_speed, by = "link_id") %>%
mutate(
avg_speed = rowMeans(select(., starts_with("avg_speed")), na.rm = TRUE),
total_records = rowSums(select(., starts_with("n_records")), na.rm = TRUE)
) %>%
select(link_id, avg_speed, total_records)
# 保存每批处理后的中间结果
save(road_speed_summary, file = paste0("./data/processed/road_speed_summary_batch_", i, ".RData"))
cat("第", i, "批数据处理完成，批次结果已保存\n")
}
for (i in seq_along(xlsx_files_batches)) {
cat("正在处理第", i, "批文件，共", length(xlsx_files_batches[[i]]), "个文件\n")
batch_data <- map_dfr(xlsx_files_batches[[i]], read_and_process_file)
batch_avg_speed <- batch_data %>%
group_by(link_id) %>%
summarise(
avg_speed_batch = mean(speed, na.rm = TRUE),
n_records_batch = n(),
.groups = "drop"
)
if (i == 1) {
# 第一次循环直接赋值
road_speed_summary <- batch_avg_speed %>%
rename(avg_speed = avg_speed_batch, total_records = n_records_batch)
} else {
# 后续批次再合并
road_speed_summary <- road_speed_summary %>%
full_join(batch_avg_speed, by = "link_id") %>%
mutate(
avg_speed = rowMeans(select(., avg_speed, avg_speed_batch), na.rm = TRUE),
total_records = rowSums(select(., total_records, n_records_batch), na.rm = TRUE)
) %>%
select(link_id, avg_speed, total_records)
}
# 保存每批处理后的中间结果
save(road_speed_summary, file = paste0("./data/processed/road_speed_summary_batch_", i, ".RData"))
cat("第", i, "批数据处理完成，批次结果已保存\n")
}
load("./data/processed/road_sf.RData")
load("./data/processed/road_speed_summary_final.RData")
save(road_speed_summary, file="./data/processed/road_speed_summary_final.RData")
list.files("./data/processed/")
load("./data/processed/road_speed_summary_final.RData")
library(dplyr)
# 确认ID类型统一
road_sf$id <- as.character(road_sf$id)
# 明确合并速度数据到空间数据
road_sf <- road_sf %>%
left_join(road_speed_summary, by = c("id" = "link_id"))
library(sf)
library(dplyr)
# 转换sf对象为data.frame进行合并（不含geometry）
road_sf_df <- road_sf %>%
st_set_geometry(NULL) %>%
mutate(id = as.character(id))
# 进行合并
road_sf_merged <- road_sf_df %>%
left_join(road_speed_summary, by = c("id" = "link_id"))
# 再重新绑定geometry
road_sf_final <- road_sf %>%
select(id, geometry) %>%
left_join(road_sf_merged, by = "id")
# 检查合并效果
head(road_sf_final)
# 删除缺失数据的道路（推荐）
road_sf <- road_sf %>%
filter(!is.na(avg_speed))
sum(road_sf_final$avg_speed, na.rm = TRUE) # 检查是否全部为NA
sum(!is.na(road_sf_final$avg_speed)) # 检查有多少条道路成功匹配到速度数据
# 删除缺失avg_speed的道路
road_sf_final <- road_sf_final %>%
filter(!is.na(avg_speed))
# 确认最终的数据量
nrow(road_sf_final)
library(spdep)
# 加载空间权重矩阵（之前创建好的）
load("./data/processed/road_listw_1.RData")
# 进行全局空间自相关分析（Moran's I）
global_moran <- moran.test(road_sf_final$avg_speed, road_listw_1, zero.policy = TRUE)
library(spdep)
# 重新计算中心坐标
coords_final <- st_coordinates(st_centroid(road_sf_final))
# 重构K最近邻空间权重矩阵 (k=2 或根据需要调整)
knn_final <- knearneigh(coords_final, k = 2)
nb_final <- knn2nb(knn_final)
# 明确生成空间权重矩阵
road_listw_final <- nb2listw(nb_final, style = "W", zero.policy = TRUE)
global_moran_final <- moran.test(road_sf_final$avg_speed, road_listw_final, zero.policy = TRUE)
# 查看结果
print(global_moran_final)
# Moran's I散点图（推荐可视化）
moran.plot(road_sf_final$avg_speed, road_listw_final, zero.policy = TRUE,
main = "Moran Scatter Plot for Road Speeds",
xlab = "Average Speed",
ylab = "Spatial Lag of Average Speed")
load("./data/processed/traffic_hourly_all.RData")
library(purrr)
library(dplyr)
# 明确列出所有traffic_hourly批次文件
batch_files <- list.files("./data/processed/",
pattern="traffic_hourly_batch_.*\\.RData",
full.names = TRUE)
# 确认文件列表不为空
print(batch_files)
# 合并所有批次数据
traffic_hourly_all <- map_dfr(batch_files, ~{
load(.x)
traffic_hourly_batch  # 确认批次保存的数据变量名
})
# 检查合并后的数据结构
head(traffic_hourly_all)
dim(traffic_hourly_all)
# 保存为最终文件
save(traffic_hourly_all, file = "./data/processed/traffic_hourly_all.RData")
library(readxl)
library(dplyr)
library(purrr)
library(lubridate)
data_dir <- "./data/raw/Q-Traffic Dataset/traffic_speed_sub-dataset.v2/split_files/"
xlsx_files <- list.files(data_dir, pattern = "\\.xlsx$", full.names = TRUE)
xlsx_files <- xlsx_files[!grepl("/~\\$", xlsx_files)]
read_hourly_speed <- function(file_path) {
data <- read_excel(file_path, col_names = FALSE) %>%
separate(col = 1, into = c("link_id", "time_index", "speed"), sep = ",", convert = TRUE) %>%
mutate(speed = as.numeric(speed),
time_index = as.integer(time_index),
link_id = as.character(link_id),
start_time = ymd_hms("2017-04-01 00:00:00"),
time = start_time + minutes((time_index - min(time_index))*15),
hour = floor_date(time, "hour")) %>%
group_by(link_id, hour) %>%
summarise(hourly_speed = mean(speed, na.rm = TRUE), .groups = "drop")
return(data)
}
batch_size <- 5
xlsx_files_batches <- split(xlsx_files, ceiling(seq_along(xlsx_files)/batch_size))
for (i in seq_along(xlsx_files_batches)) {
cat("正在处理第", i, "批\n")
traffic_hourly_batch <- map_dfr(xlsx_files_batches[[i]], read_hourly_speed)
save(traffic_hourly_batch,
file = paste0("./data/processed/traffic_hourly_batch_", i, ".RData"))
cat("第", i, "批数据处理完成\n")
}
batch_files <- list.files("./data/processed/",
pattern="traffic_hourly_batch_.*\\.RData",
full.names = TRUE)
traffic_hourly_all <- map_dfr(batch_files, ~{
load(.x)
traffic_hourly_batch
})
save(traffic_hourly_all, file = "./data/processed/traffic_hourly_all.RData")
load("./data/processed/traffic_hourly_all.RData")
# 检查数据结构：必须是包含 (hour, link_id, hourly_speed) 字段的数据框
head(traffic_hourly_all)
library(dplyr)
library(tidyr)
# 转换为STARIMA矩阵形式
traffic_matrix <- traffic_hourly_all %>%
pivot_wider(names_from = link_id, values_from = hourly_speed) %>%
arrange(hour)
# 检查数据矩阵（行:时间, 列:道路）
head(traffic_matrix)
dim(traffic_matrix)
library(spdep)
library(forecast)
# 假设目标道路id为"1525866939"
target_road <- "1525866939"
target_series <- traffic_matrix[[target_road]]
# 确认数据长度
length(target_series)
# 已构建空间滞后矩阵 spatial_lag_matrix
spatial_lag_target <- spatial_lag_matrix[, target_road]
library(spdep)
# 加载空间权重矩阵（确保之前保存的存在）
load("./data/processed/road_listw_final.RData")
# 加载空间权重矩阵（确保之前保存的存在）
load("./data/processed/road_listw_2.RData")
# 确保traffic_matrix已经存在（上一阶段已创建）
head(traffic_matrix)
# 已构建空间滞后矩阵 spatial_lag_matrix
spatial_lag_target <- spatial_lag_matrix[, target_road]
# 明确提取除时间列之外的道路数据
spatial_data <- traffic_matrix %>% select(-hour)
# 明确创建空间滞后矩阵
spatial_lag_matrix <- apply(spatial_data, 1, function(x) {
lag.listw(road_listw_final, x, zero.policy = TRUE)
}) %>% t()
# 检查空间滞后矩阵结构
dim(spatial_lag_matrix)
# 设置明确的列名（与道路id匹配）
colnames(spatial_lag_matrix) <- colnames(spatial_data)
# 提取目标道路空间滞后序列
spatial_lag_target <- spatial_lag_matrix[, target_road]
# 构建未来24小时空间滞后预测项
future_spatial_lag <- rep(tail(spatial_lag_target, 1), 24)
# 拟合STARIMA模型并预测（推荐立即执行）
library(forecast)
starima_model <- auto.arima(target_series, xreg = spatial_lag_target)
forecast_result <- forecast(starima_model, xreg = future_spatial_lag, h = 24)
# 显示并绘制预测结果
print(forecast_result)
plot(forecast_result, main = "STARIMA Model Traffic Speed Forecast (Next 24 Hours)",
xlab = "Future Time (Hours)",
ylab = "Predicted Traffic Speed")
library(lubridate)
library(dplyr)
# 假设traffic_matrix数据已存在
traffic_matrix <- traffic_matrix %>% arrange(hour)
# 设定训练集和测试集的分割日期
train_end_date <- ymd_hms("2017-05-12 23:00:00")
# 明确划分训练集
train_data <- traffic_matrix %>% filter(hour <= train_end_date)
# 明确划分测试集
test_data <- traffic_matrix %>% filter(hour > train_end_date)
# 检查划分后的数据时段
range(train_data$hour)
range(test_data$hour)
library(dplyr)
library(tidyr)
train_matrix <- train_data %>%
pivot_longer(-hour, names_to = "link_id", values_to = "speed") %>%
pivot_wider(names_from = link_id, values_from = speed) %>%
arrange(hour)
# 检查训练数据矩阵
head(train_matrix)
library(dplyr)
library(tidyr)
train_matrix <- train_data %>%
pivot_longer(-hour, names_to = "link_id", values_to = "speed") %>%
pivot_wider(names_from = link_id, values_from = speed) %>%
arrange(hour)
# 检查训练数据矩阵
head(train_matrix)
library(forecast)
# 提取目标道路的训练集序列
target_series_train <- train_matrix[["1525866939"]]
# 提取目标道路的空间滞后序列
spatial_lag_train <- train_spatial_lag[, "1525866939"]
# 提取训练集数据（不含hour）
train_spatial_data <- train_matrix %>% select(-hour)
# 明确创建空间滞后矩阵
train_spatial_lag <- apply(train_spatial_data, 1, function(x){
lag.listw(road_listw_final, x, zero.policy = TRUE)
}) %>% t()
# 设置明确列名
colnames(train_spatial_lag) <- colnames(train_spatial_data)
library(forecast)
# 提取目标道路的训练集序列
target_series_train <- train_matrix[["1525866939"]]
# 提取目标道路的空间滞后序列
spatial_lag_train <- train_spatial_lag[, "1525866939"]
# 明确STARIMA模型拟合
starima_model <- auto.arima(target_series_train, xreg = spatial_lag_train)
# 查看模型拟合结果
summary(starima_model)
# 明确构建测试集时空数据矩阵
test_matrix <- test_data %>%
pivot_longer(-hour, names_to = "link_id", values_to = "speed") %>%
pivot_wider(names_from = link_id, values_from = speed) %>%
arrange(hour)
# 提取测试集数据（不含hour）
test_spatial_data <- test_matrix %>% select(-hour)
# 测试集空间滞后矩阵计算
test_spatial_lag <- apply(test_spatial_data, 1, function(x){
lag.listw(road_listw_final, x, zero.policy = TRUE)
}) %>% t()
# 设置明确列名
colnames(test_spatial_lag) <- colnames(test_spatial_data)
# 提取目标道路空间滞后项（测试集）
spatial_lag_test <- test_spatial_lag[, "1525866939"]
# 使用STARIMA模型预测测试集
forecast_test <- forecast(starima_model, xreg = spatial_lag_test)
# 查看预测结果
print(forecast_test)
plot(forecast_test,
main = "STARIMA Model Forecast on Test Set",
xlab = "Time (Hours)",
ylab = "Predicted Traffic Speed")
# 明确计算预测误差指标
rmse <- sqrt(mean((actual_test - forecast_test$mean)^2, na.rm = TRUE))
mae <- mean(abs(actual_test - forecast_test$mean), na.rm = TRUE)
mape <- mean(abs((actual_test - forecast_test$mean)/actual_test), na.rm = TRUE) * 100
cat("测试集预测RMSE:", rmse, "\n")
cat("测试集预测MAE:", mae, "\n")
cat("测试集预测MAPE:", mape, "%\n")
